{
  
    
        "post0": {
            "title": "Title",
            "content": "Intro and Downloading data . Welcome to this exciting journey aboard Titanic Spaceship! . This is tutorial is almost a complete copy of Jeremy Howard&#39;s excellent Linear model and neural net from scratch kaggle notebook which was part of the 2022 version of Deep Learning for Coders course by FastAI. . This also borrows heavily from Spaceship Titanic: A complete guide notebook by Samuel Cortinhas which is based on the dataset I&#39;m gonna use to build by neural network. . Lastly and very importantly, Neural Network from Scratch series by sentdex youtuber channel&#39;s Harrison and Daniel was instrumental in broadening my understanding. I have burrowed their way of coding neural network layers as classes. You can access their video and the book here . My explanations/comments are going to be minimal, because everything is explained quite well in the resources mentioned above. For neural net foundations(which is the primary aim of this blog/notebook). please refer to course.fast.ai by Jeremy &amp; Co. and the nnfs.io series. . Objective of dataset: To predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic&#39;s collision with a spacetime anomaly. | . My Aim : is to replicate Jeremy&#39;s work on a different dataset(his work was based on original Titanic dataset), as part of my learning. | . Let&#39;s go conquer a neural network then! . Description of the features of the dataset, copied from the competition page: . PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always. . | HomePlanet - The planet the passenger departed from, typically their planet of permanent residence. . | CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins. . | Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard. . | . Destination - The planet the passenger will be debarking to. . | Age - The age of the passenger. . | VIP - Whether the passenger has paid for special VIP service during the voyage. . | RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic&#39;s many luxury amenities. . | Name - The first and last names of the passenger. . | Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict. . | . import numpy as np import pandas as pd import torch import kaggle import os from pathlib import Path . Downloading the data from Titanic-spaceship competition via the kaggle API: . path = Path(&#39;spaceship-titanic&#39;) if not path.exists(): import zipfile,kaggle kaggle.api.competition_download_cli(str(path)) zipfile.ZipFile(f&#39;{path}.zip&#39;).extractall(path) . Setting display options for numpy, pandas and pytorch to widen the output frames: . np.set_printoptions(linewidth=140) torch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7) pd.set_option(&#39;display.width&#39;, 140) . Cleaning the data . Looking at some samples from the dataset: . df = pd.read_csv(path/&#39;train.csv&#39;) df.head() . PassengerId HomePlanet CryoSleep Cabin Destination Age VIP RoomService FoodCourt ShoppingMall Spa VRDeck Name Transported . 0 0001_01 | Europa | False | B/0/P | TRAPPIST-1e | 39.0 | False | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | Maham Ofracculy | False | . 1 0002_01 | Earth | False | F/0/S | TRAPPIST-1e | 24.0 | False | 109.0 | 9.0 | 25.0 | 549.0 | 44.0 | Juanna Vines | True | . 2 0003_01 | Europa | False | A/0/S | TRAPPIST-1e | 58.0 | True | 43.0 | 3576.0 | 0.0 | 6715.0 | 49.0 | Altark Susent | False | . 3 0003_02 | Europa | False | A/0/S | TRAPPIST-1e | 33.0 | False | 0.0 | 1283.0 | 371.0 | 3329.0 | 193.0 | Solam Susent | False | . 4 0004_01 | Earth | False | F/1/S | TRAPPIST-1e | 16.0 | False | 303.0 | 70.0 | 151.0 | 565.0 | 2.0 | Willy Santantines | True | . Exploring missing values and filling them with the Mode of the respective column: . df.isna().sum() . PassengerId 0 HomePlanet 201 CryoSleep 217 Cabin 199 Destination 182 Age 179 VIP 203 RoomService 181 FoodCourt 183 ShoppingMall 208 Spa 183 VRDeck 188 Name 200 Transported 0 dtype: int64 . modes = df.mode().iloc[0] modes . PassengerId 0001_01 HomePlanet Earth CryoSleep False Cabin G/734/S Destination TRAPPIST-1e Age 24.0 VIP False RoomService 0.0 FoodCourt 0.0 ShoppingMall 0.0 Spa 0.0 VRDeck 0.0 Name Alraium Disivering Transported True Name: 0, dtype: object . df.fillna(modes, inplace=True) . df.isna().sum() . PassengerId 0 HomePlanet 0 CryoSleep 0 Cabin 0 Destination 0 Age 0 VIP 0 RoomService 0 FoodCourt 0 ShoppingMall 0 Spa 0 VRDeck 0 Name 0 Transported 0 dtype: int64 . Exploratory Data Analysis &amp; Feature Engineering . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns plt.style.use(&#39;ggplot&#39;) . plt.figure(figsize=(6,6)) # Pie plot df[&#39;Transported&#39;].value_counts().plot.pie(explode=[0.03,0.03], autopct=&#39;%1.1f%%&#39;, shadow=True, textprops={&#39;fontsize&#39;:16}).set_title(&quot;Target distribution&quot;) . Text(0.5, 1.0, &#39;Target distribution&#39;) . Target varaible is quite balanced, hence we don&#39;t need to perform over/undersampling. . Now let&#39;s describe categorical features: . df.describe(include=[object]) . PassengerId HomePlanet Cabin Destination Name . count 8693 | 8693 | 8693 | 8693 | 8693 | . unique 8693 | 3 | 6560 | 3 | 8473 | . top 0001_01 | Earth | G/734/S | TRAPPIST-1e | Alraium Disivering | . freq 1 | 4803 | 207 | 6097 | 202 | . Let&#39;s now replace the strings in these categorical features by numbers. Pandas offers a get_dummies method to convert these to numbers so that we can multiply them with weights. It&#39;s basically one-hot coding, letting the model know the unqiue levels available in a particular. . We only process HomePlanet and Destination via get_dummiesbecause others simply have too many unique values (aka levels). . df = pd.get_dummies(df, columns=[&quot;HomePlanet&quot;, &quot;Destination&quot;]) df.columns . Index([&#39;PassengerId&#39;, &#39;CryoSleep&#39;, &#39;Cabin&#39;, &#39;Age&#39;, &#39;VIP&#39;, &#39;RoomService&#39;, &#39;FoodCourt&#39;, &#39;ShoppingMall&#39;, &#39;Spa&#39;, &#39;VRDeck&#39;, &#39;Name&#39;, &#39;Transported&#39;, &#39;HomePlanet_Earth&#39;, &#39;HomePlanet_Europa&#39;, &#39;HomePlanet_Mars&#39;, &#39;Destination_55 Cancri e&#39;, &#39;Destination_PSO J318.5-22&#39;, &#39;Destination_TRAPPIST-1e&#39;], dtype=&#39;object&#39;) . Our dummy columns are visible at the end of the dataframe! . Looking at numerical features: . df.describe(include=(np.number)) . Age RoomService FoodCourt ShoppingMall Spa VRDeck HomePlanet_Earth HomePlanet_Europa HomePlanet_Mars Destination_55 Cancri e Destination_PSO J318.5-22 Destination_TRAPPIST-1e . count 8693.000000 | 8693.000000 | 8693.000000 | 8693.000000 | 8693.000000 | 8693.000000 | 8693.000000 | 8693.000000 | 8693.000000 | 8693.000000 | 8693.000000 | 8693.000000 | . mean 28.728517 | 220.009318 | 448.434027 | 169.572300 | 304.588865 | 298.261820 | 0.552514 | 0.245140 | 0.202347 | 0.207063 | 0.091568 | 0.701369 | . std 14.355438 | 660.519050 | 1595.790627 | 598.007164 | 1125.562559 | 1134.126417 | 0.497263 | 0.430195 | 0.401772 | 0.405224 | 0.288432 | 0.457684 | . min 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 20.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 50% 27.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | . 75% 37.000000 | 41.000000 | 61.000000 | 22.000000 | 53.000000 | 40.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | . max 79.000000 | 14327.000000 | 29813.000000 | 23492.000000 | 22408.000000 | 24133.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | . Samuel&#39;s notebook uncovered the following useful insight regarding Age. Let&#39;s visualize the feature first: . plt.figure(figsize=(10,4)) # Histogram sns.histplot(data=df, x=&#39;Age&#39;, hue=&#39;Transported&#39;, binwidth=1, kde=True) # Aesthetics plt.title(&#39;Age distribution&#39;) plt.xlabel(&#39;Age (years)&#39;) . Text(0.5, 0, &#39;Age (years)&#39;) . Notes and insights by Samuel: . Notes: . 0-18 year olds were more likely to be transported than not. | 18-25 year olds were less likely to be transported than not. | Over 25 year olds were about equally likely to be transported than not. | . Insight: . Create a new feature that indicates whether the passanger is a child, adolescent or adult. | . p_groups = [&#39;child&#39;, &#39;young&#39;, &#39;adult&#39;] df[&#39;age_group&#39;] = np.nan . df.loc[df[&#39;Age&#39;] &lt; 18, &#39;age_group&#39;] = p_groups[0] df.loc[(df[&#39;Age&#39;] &gt;= 18) &amp; (df[&#39;Age&#39;] &lt;= 25), &#39;age_group&#39;] = p_groups[1] df.loc[df[&#39;Age&#39;] &gt; 25, &#39;age_group&#39;] = p_groups[2] . df.head() . PassengerId CryoSleep Cabin Age VIP RoomService FoodCourt ShoppingMall Spa VRDeck Name Transported HomePlanet_Earth HomePlanet_Europa HomePlanet_Mars Destination_55 Cancri e Destination_PSO J318.5-22 Destination_TRAPPIST-1e age_group . 0 0001_01 | False | B/0/P | 39.0 | False | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | Maham Ofracculy | False | 0 | 1 | 0 | 0 | 0 | 1 | adult | . 1 0002_01 | False | F/0/S | 24.0 | False | 109.0 | 9.0 | 25.0 | 549.0 | 44.0 | Juanna Vines | True | 1 | 0 | 0 | 0 | 0 | 1 | young | . 2 0003_01 | False | A/0/S | 58.0 | True | 43.0 | 3576.0 | 0.0 | 6715.0 | 49.0 | Altark Susent | False | 0 | 1 | 0 | 0 | 0 | 1 | adult | . 3 0003_02 | False | A/0/S | 33.0 | False | 0.0 | 1283.0 | 371.0 | 3329.0 | 193.0 | Solam Susent | False | 0 | 1 | 0 | 0 | 0 | 1 | adult | . 4 0004_01 | False | F/1/S | 16.0 | False | 303.0 | 70.0 | 151.0 | 565.0 | 2.0 | Willy Santantines | True | 1 | 0 | 0 | 0 | 0 | 1 | child | . Now we need dummies for age_group as well: . df = pd.get_dummies(df, columns=[&quot;age_group&quot;]) df.head() . PassengerId CryoSleep Cabin Age VIP RoomService FoodCourt ShoppingMall Spa VRDeck ... Transported HomePlanet_Earth HomePlanet_Europa HomePlanet_Mars Destination_55 Cancri e Destination_PSO J318.5-22 Destination_TRAPPIST-1e age_group_adult age_group_child age_group_young . 0 0001_01 | False | B/0/P | 39.0 | False | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | False | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | . 1 0002_01 | False | F/0/S | 24.0 | False | 109.0 | 9.0 | 25.0 | 549.0 | 44.0 | ... | True | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | . 2 0003_01 | False | A/0/S | 58.0 | True | 43.0 | 3576.0 | 0.0 | 6715.0 | 49.0 | ... | False | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | . 3 0003_02 | False | A/0/S | 33.0 | False | 0.0 | 1283.0 | 371.0 | 3329.0 | 193.0 | ... | False | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | . 4 0004_01 | False | F/1/S | 16.0 | False | 303.0 | 70.0 | 151.0 | 565.0 | 2.0 | ... | True | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | . 5 rows × 21 columns . exp_feats=[&#39;RoomService&#39;, &#39;FoodCourt&#39;, &#39;ShoppingMall&#39;, &#39;Spa&#39;, &#39;VRDeck&#39;] # Plot expenditure features fig=plt.figure(figsize=(10,20)) for i, var_name in enumerate(exp_feats): # Left plot ax=fig.add_subplot(5,2,2*i+1) sns.histplot(data=df, x=var_name, axes=ax, bins=30, kde=False, hue=&#39;Transported&#39;) ax.set_title(var_name) # Right plot (truncated) ax=fig.add_subplot(5,2,2*i+2) sns.histplot(data=df, x=var_name, axes=ax, bins=30, kde=True, hue=&#39;Transported&#39;) plt.ylim([0,100]) ax.set_title(var_name) fig.tight_layout() # Improves appearance a bit plt.show() . Insights: . Luxuries such as VR decl and Spa were clearly more used by people who were NOT transported. | Need to create a log form for all the above to reduce the skew of the distributions. | . for f in exp_feats: df[f&#39;log{f}&#39;] = np.log(df[f]+1) . df.head() . PassengerId CryoSleep Cabin Age VIP RoomService FoodCourt ShoppingMall Spa VRDeck ... Destination_PSO J318.5-22 Destination_TRAPPIST-1e age_group_adult age_group_child age_group_young logRoomService logFoodCourt logShoppingMall logSpa logVRDeck . 0 0001_01 | False | B/0/P | 39.0 | False | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0 | 1 | 1 | 0 | 0 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 1 0002_01 | False | F/0/S | 24.0 | False | 109.0 | 9.0 | 25.0 | 549.0 | 44.0 | ... | 0 | 1 | 0 | 0 | 1 | 4.700480 | 2.302585 | 3.258097 | 6.309918 | 3.806662 | . 2 0003_01 | False | A/0/S | 58.0 | True | 43.0 | 3576.0 | 0.0 | 6715.0 | 49.0 | ... | 0 | 1 | 1 | 0 | 0 | 3.784190 | 8.182280 | 0.000000 | 8.812248 | 3.912023 | . 3 0003_02 | False | A/0/S | 33.0 | False | 0.0 | 1283.0 | 371.0 | 3329.0 | 193.0 | ... | 0 | 1 | 1 | 0 | 0 | 0.000000 | 7.157735 | 5.918894 | 8.110728 | 5.267858 | . 4 0004_01 | False | F/1/S | 16.0 | False | 303.0 | 70.0 | 151.0 | 565.0 | 2.0 | ... | 0 | 1 | 0 | 1 | 0 | 5.717028 | 4.262680 | 5.023881 | 6.338594 | 1.098612 | . 5 rows × 26 columns . Finally, we are splitting PassengerId as per its data description: . df[&#39;Group&#39;] = df[&#39;PassengerId&#39;].apply(lambda x: x.split(&#39;_&#39;)[0]).astype(int) . df.columns . Index([&#39;PassengerId&#39;, &#39;CryoSleep&#39;, &#39;Cabin&#39;, &#39;Age&#39;, &#39;VIP&#39;, &#39;RoomService&#39;, &#39;FoodCourt&#39;, &#39;ShoppingMall&#39;, &#39;Spa&#39;, &#39;VRDeck&#39;, &#39;Name&#39;, &#39;Transported&#39;, &#39;HomePlanet_Earth&#39;, &#39;HomePlanet_Europa&#39;, &#39;HomePlanet_Mars&#39;, &#39;Destination_55 Cancri e&#39;, &#39;Destination_PSO J318.5-22&#39;, &#39;Destination_TRAPPIST-1e&#39;, &#39;age_group_adult&#39;, &#39;age_group_child&#39;, &#39;age_group_young&#39;, &#39;logRoomService&#39;, &#39;logFoodCourt&#39;, &#39;logShoppingMall&#39;, &#39;logSpa&#39;, &#39;logVRDeck&#39;, &#39;Group&#39;], dtype=&#39;object&#39;) . added_cols = [&#39;HomePlanet_Earth&#39;, &#39;HomePlanet_Europa&#39;,&#39;HomePlanet_Mars&#39;, &#39;Destination_55 Cancri e&#39;, &#39;Destination_PSO J318.5-22&#39;, &#39;Destination_TRAPPIST-1e&#39;,&#39;age_group_adult&#39;, &#39;age_group_child&#39;, &#39;age_group_young&#39;, &#39;logRoomService&#39;, &#39;logFoodCourt&#39;, &#39;logShoppingMall&#39;, &#39;logSpa&#39;, &#39;logVRDeck&#39;, &#39;Group&#39;] . df[added_cols].head() . HomePlanet_Earth HomePlanet_Europa HomePlanet_Mars Destination_55 Cancri e Destination_PSO J318.5-22 Destination_TRAPPIST-1e age_group_adult age_group_child age_group_young logRoomService logFoodCourt logShoppingMall logSpa logVRDeck Group . 0 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1 | . 1 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 4.700480 | 2.302585 | 3.258097 | 6.309918 | 3.806662 | 2 | . 2 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 3.784190 | 8.182280 | 0.000000 | 8.812248 | 3.912023 | 3 | . 3 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0.000000 | 7.157735 | 5.918894 | 8.110728 | 5.267858 | 3 | . 4 1 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 5.717028 | 4.262680 | 5.023881 | 6.338594 | 1.098612 | 4 | . What about CryoSleep? . df.CryoSleep.unique() . array([False, True]) . It&#39;s boolean, so we can mulitply with weights. VIP looks the same. . indep_cols = [&#39;Age&#39;, &#39;CryoSleep&#39;, &#39;VIP&#39;] + added_cols . Setting up a linear model . Single layer neural network with one neuron: . np.random.seed(442) weights = np.random.randn(len(indep_cols), 1) bias = np.random.randn(1) . preds = np.dot(df[indep_cols].values, weights) . preds.shape . (8693, 1) . trn_indep = np.array(df[indep_cols], dtype=&#39;float32&#39;) trn_dep = np.array(df[&#39;Transported&#39;], dtype=&#39;float32&#39;) . print(trn_indep.shape) print(trn_dep.shape) . (8693, 18) (8693,) . Single layer neural network with three neurons: . np.random.seed(442) weights = np.random.randn(len(indep_cols), 3) #three neurons in this layer bias = np.random.randn(1, 3) . bias . array([[ 0.01176998, -0.85427749, -0.99987562]]) . preds = np.dot(df[indep_cols].values, weights) + bias . preds.shape . (8693, 3) . preds[:10] . array([[-48.06963390253141, -18.290557540354897, 25.89840533681499], [-37.91377002769904, -7.504076537856783, 19.929613327882404], [-77.52103870477566, -16.739210482170467, 48.30474706150288], [-49.956656061980745, -2.8342719146155777, 27.74823775227435], [-27.921019655098725, -0.8518280063674349, 7.996057055720733], [-58.52447309612352, -11.829816864262263, 30.01637869888577], [-43.57861207919503, -3.600338170392331, 14.143030534080323], [-42.93381590139605, -11.247805747916502, 13.576656755604825], [-53.425588340898884, -4.639261738916223, 17.59311661596172], [-25.380443498116176, -10.613520203166267, 1.9029395296299394]], dtype=object) . A deeper neural network with 2 hidden layers and an output layer: . trn_dep = torch.tensor(trn_dep, dtype=torch.long) trn_indep = torch.tensor(trn_indep, dtype=torch.float) . trn_indep.shape . torch.Size([8693, 18]) . trn_dep.shape . torch.Size([8693]) . trn_indep[:5] . tensor([[39.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000], [24.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 4.7005, 2.3026, 3.2581, 6.3099, 3.8067, 2.0000], [58.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 3.7842, 8.1823, 0.0000, 8.8122, 3.9120, 3.0000], [33.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 7.1577, 5.9189, 8.1107, 5.2679, 3.0000], [16.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 5.7170, 4.2627, 5.0239, 6.3386, 1.0986, 4.0000]]) . . vals,indices = trn_indep.max(dim=0) trn_indep = trn_indep / vals . Next let&#39;s define our layers, I&#39;m going to define them as classes, so that we can reuse objects of each classes when needed. . class linearlayer: def __init__(self, n_inputs, n_neurons): self.weights = ((torch.rand(n_inputs, n_neurons)-0.3)/n_neurons)*4 self.weights = self.weights.requires_grad_() self.biases = ((torch.rand(1, n_neurons))-0.5)*0.1 self.biases = self.biases.requires_grad_() def forward(self, inputs): self.output = inputs@self.weights + self.biases . class ReLU_act: def forward(self, inputs): self.output = torch.clip(inputs, 0.) . class Softmax_act: def forward(self, inputs): exp_values = torch.exp(inputs) probs = exp_values/torch.sum(exp_values, axis=1, keepdims=True) self.output = probs . n_inputs=len(indep_cols) n_hidden=10 inputs=trn_indep y_true=trn_dep . n_inputs . 18 . Initializing the parameters of our network: . layer1 = linearlayer(n_inputs, n_hidden) relu1 = ReLU_act() layer2 = linearlayer(n_hidden, n_hidden) relu2 = ReLU_act() layer3 = linearlayer(n_hidden, 2) wandbs = layer1.weights, layer2.weights, layer3.weights, layer1.biases, layer2.biases, layer3.biases print(&#39;These are our weights and biases:&#39;) print(wandbs) layer1.forward(inputs) print(&quot; n&quot;) print(&#39;These are our l1 outputs:&#39;) print(layer1.output) relu1.forward(layer1.output) print(&quot; n&quot;) print(&#39;These are our r1 outputs:&#39;) print(relu1.output) layer2.forward(relu1.output) print(&quot; n&quot;) print(&#39;These are our l2 outputs:&#39;) print(layer2.output) relu2.forward(layer2.output) print(&quot; n&quot;) print(&#39;These are our r2 outputs:&#39;) print(relu2.output) layer3.forward(relu2.output) print(&quot; n&quot;) print(&#39;These are our l3 outputs:&#39;) print(layer3.output) softmax = Softmax_act() softmax.forward(layer3.output) print(&quot; n&quot;) print(&#39;These are our softmax outputs:&#39;) print(softmax.output) . These are our weights and biases: (tensor([[ -0.0002, 0.2331, 0.0117, 0.2600, 0.2774, 0.1307, 0.1950, 0.2057, 0.1399, -0.0986], [ 0.1946, -0.0915, 0.0643, 0.0087, 0.2372, 0.2070, 0.2365, 0.2561, 0.2278, -0.0841], [ 0.0705, -0.1123, -0.0432, 0.2374, 0.1913, 0.1934, 0.0814, -0.0839, -0.1167, 0.2246], [ 0.0705, 0.1724, 0.1248, 0.0990, 0.1931, -0.0708, -0.0022, 0.2181, 0.0299, 0.1734], [ 0.1153, -0.1066, -0.0893, 0.1900, 0.2777, 0.1359, 0.1473, 0.1143, 0.2712, -0.0556], [ 0.1509, 0.1151, 0.0681, -0.0028, 0.2353, 0.1948, 0.0870, 0.0006, -0.0163, 0.0389], [ -0.0553, 0.2512, 0.0427, 0.2267, -0.0093, 0.1136, -0.1013, 0.1188, -0.0013, 0.1935], [ 0.1943, 0.2582, 0.1152, 0.2282, 0.1577, 0.0751, 0.0589, 0.0211, -0.0506, 0.1875], [ 0.0339, 0.1409, 0.0418, 0.1518, 0.0863, 0.0752, -0.0287, 0.0064, -0.0014, 0.0103], [ -0.1056, 0.2283, 0.1243, 0.1812, 0.1591, 0.1607, 0.1488, -0.0419, -0.0072, -0.0633], [ -0.1127, -0.0293, 0.1272, 0.0205, 0.0115, 0.2260, -0.0251, 0.2198, -0.0274, 0.0513], [ 0.0951, 0.0732, 0.2631, 0.1341, -0.0821, 0.0232, 0.1464, 0.0473, 0.1390, -0.0458], [ 0.1315, 0.0439, 0.1412, -0.0664, 0.0940, 0.1332, -0.0286, 0.0506, 0.0248, 0.0755], [ 0.2530, 0.2420, 0.2765, 0.2048, -0.0882, -0.0035, 0.1577, 0.0605, 0.0726, 0.2733], [ 0.2079, 0.0583, 0.0246, 0.1822, -0.0031, 0.1719, -0.0781, 0.2127, 0.2108, 0.0760], [ -0.0597, -0.0759, 0.1393, 0.2165, 0.2028, 0.0627, -0.0170, -0.0651, 0.0579, 0.0773], [ 0.1473, -0.0495, 0.2317, 0.0157, -0.1144, -0.0419, 0.1788, 0.2388, 0.1428, 0.0085], [ 0.0061, 0.2621, 0.2392, 0.0263, -0.1017, -0.0018, 0.1258, 0.1266, 0.1345, -0.0650]], requires_grad=True), tensor([[-0.0621, -0.0691, 0.2171, -0.0623, 0.1145, 0.0525, 0.2408, 0.2170, 0.1916, 0.0629], [-0.0341, -0.0009, -0.0881, 0.1173, 0.0511, -0.0289, 0.1675, 0.2559, 0.0796, -0.0451], [ 0.1161, 0.0154, -0.0154, -0.0897, 0.1087, -0.0746, -0.0648, -0.0229, 0.1991, 0.2154], [ 0.0914, 0.0735, -0.0821, -0.0773, 0.0863, 0.0284, -0.0972, 0.1577, 0.1829, -0.0549], [ 0.0412, 0.1230, 0.2663, 0.0677, 0.2552, -0.0593, 0.0626, 0.1118, -0.0184, 0.1215], [-0.0723, 0.1951, 0.0814, 0.1907, -0.0819, 0.0893, 0.2131, -0.0612, 0.0357, 0.0940], [ 0.2410, 0.0710, -0.0211, 0.1043, 0.0940, 0.0236, -0.1108, -0.0611, 0.2035, -0.0306], [ 0.1431, 0.2022, 0.1448, -0.1078, -0.0829, 0.1917, 0.2769, 0.1859, 0.0531, 0.1962], [ 0.0767, 0.2175, 0.1521, -0.0540, 0.0504, 0.2353, 0.2760, -0.1082, 0.1134, 0.2455], [ 0.2477, 0.2034, 0.1353, 0.0091, -0.0367, 0.2536, 0.0904, 0.1440, -0.1072, -0.0786]], requires_grad=True), tensor([[ 0.6042, 0.9942], [ 1.1100, 0.6981], [ 0.2269, -0.0373], [ 0.8578, 0.3621], [ 1.1303, 0.1530], [ 1.2218, 0.6881], [ 0.4251, 0.7249], [-0.3872, 0.6244], [ 1.3207, -0.3660], [ 0.7619, 0.8244]], requires_grad=True), tensor([[ 0.0031, 0.0179, -0.0161, -0.0428, -0.0240, -0.0101, -0.0082, 0.0291, 0.0329, -0.0085]], requires_grad=True), tensor([[-0.0072, 0.0279, -0.0105, -0.0237, -0.0117, 0.0383, 0.0107, -0.0042, 0.0435, 0.0144]], requires_grad=True), tensor([[-0.0298, -0.0055]], requires_grad=True)) These are our l1 outputs: tensor([[ 0.0465, 0.3955, 0.0665, 0.6085, 0.6360, 0.4262, 0.3555, 0.2094, 0.3645, -0.1657], [ 0.4090, 0.5032, 0.7315, 0.6355, 0.3676, 0.2011, 0.2193, 0.5196, 0.4299, 0.2742], [ 0.3745, 0.4630, 0.5139, 1.2414, 0.9952, 0.7398, 0.6521, 0.2783, 0.4553, 0.3533], [ 0.3732, 0.4930, 0.5060, 1.0217, 0.6563, 0.5438, 0.4839, 0.4328, 0.6498, 0.1433], [ 0.2600, 0.4513, 0.6046, 0.5558, 0.4570, 0.4457, -0.0068, 0.6613, 0.2647, 0.4525], [ 0.2802, 0.9090, 0.5997, 0.8559, 0.7023, 0.2610, 0.3909, 0.3403, 0.1594, 0.4420], [ 0.2624, 0.8340, 0.5346, 0.6196, 0.4794, 0.2714, 0.2643, 0.3717, 0.1908, 0.3143], ..., [ 0.2636, 0.7582, 0.6401, 0.8374, 0.4956, 0.4299, 0.6259, 0.4212, 0.6110, 0.0037], [ 0.3290, 0.7838, 0.6694, 0.7983, 0.3901, 0.3895, 0.6834, 0.4900, 0.6283, 0.0081], [ 0.2696, 0.7915, 0.7026, 1.2959, 0.6626, 0.6849, 0.6939, 0.4754, 0.5521, 0.4695], [ 0.5637, 0.7453, 0.7932, 0.5127, 0.4434, 0.2524, 0.6015, 0.7450, 0.5453, 0.1351], [ 0.1594, 0.9366, 0.5459, 0.6524, 0.4158, 0.3292, 0.2400, 0.5606, 0.3965, 0.0767], [ 0.2173, 0.8266, 0.7594, 0.9643, 0.3819, 0.4520, 0.6310, 0.6241, 0.6840, 0.1978], [ 0.3641, 0.8806, 0.6636, 0.7896, 0.4980, 0.4865, 0.6539, 0.4849, 0.6162, 0.0276]], grad_fn=&lt;AddBackward0&gt;) These are our r1 outputs: tensor([[0.0465, 0.3955, 0.0665, 0.6085, 0.6360, 0.4262, 0.3555, 0.2094, 0.3645, 0.0000], [0.4090, 0.5032, 0.7315, 0.6355, 0.3676, 0.2011, 0.2193, 0.5196, 0.4299, 0.2742], [0.3745, 0.4630, 0.5139, 1.2414, 0.9952, 0.7398, 0.6521, 0.2783, 0.4553, 0.3533], [0.3732, 0.4930, 0.5060, 1.0217, 0.6563, 0.5438, 0.4839, 0.4328, 0.6498, 0.1433], [0.2600, 0.4513, 0.6046, 0.5558, 0.4570, 0.4457, 0.0000, 0.6613, 0.2647, 0.4525], [0.2802, 0.9090, 0.5997, 0.8559, 0.7023, 0.2610, 0.3909, 0.3403, 0.1594, 0.4420], [0.2624, 0.8340, 0.5346, 0.6196, 0.4794, 0.2714, 0.2643, 0.3717, 0.1908, 0.3143], ..., [0.2636, 0.7582, 0.6401, 0.8374, 0.4956, 0.4299, 0.6259, 0.4212, 0.6110, 0.0037], [0.3290, 0.7838, 0.6694, 0.7983, 0.3901, 0.3895, 0.6834, 0.4900, 0.6283, 0.0081], [0.2696, 0.7915, 0.7026, 1.2959, 0.6626, 0.6849, 0.6939, 0.4754, 0.5521, 0.4695], [0.5637, 0.7453, 0.7932, 0.5127, 0.4434, 0.2524, 0.6015, 0.7450, 0.5453, 0.1351], [0.1594, 0.9366, 0.5459, 0.6524, 0.4158, 0.3292, 0.2400, 0.5606, 0.3965, 0.0767], [0.2173, 0.8266, 0.7594, 0.9643, 0.3819, 0.4520, 0.6310, 0.6241, 0.6840, 0.1978], [0.3641, 0.8806, 0.6636, 0.7896, 0.4980, 0.4865, 0.6539, 0.4849, 0.6162, 0.0276]], grad_fn=&lt;ClampBackward1&gt;) These are our l2 outputs: tensor([[ 0.1788, 0.3784, 0.1961, 0.0859, 0.2355, 0.1763, 0.2745, 0.2243, 0.3367, 0.2174], [ 0.3220, 0.4116, 0.2579, -0.0955, 0.2618, 0.2804, 0.4132, 0.4018, 0.5158, 0.3829], [ 0.4339, 0.6410, 0.3890, 0.0903, 0.4595, 0.3141, 0.4068, 0.4590, 0.6507, 0.3687], [ 0.3566, 0.5638, 0.3251, 0.0046, 0.3596, 0.3282, 0.4666, 0.3938, 0.6289, 0.4177], [ 0.2960, 0.4863, 0.3065, -0.0497, 0.1770, 0.3261, 0.4764, 0.4248, 0.3754, 0.3667], [ 0.3670, 0.4384, 0.2241, 0.0423, 0.3642, 0.2121, 0.3428, 0.5588, 0.5101, 0.2422], [ 0.2763, 0.3741, 0.1834, 0.0262, 0.2649, 0.2036, 0.3613, 0.4662, 0.4422, 0.2451], ..., [ 0.3498, 0.4886, 0.2095, 0.0291, 0.3451, 0.2555, 0.4172, 0.3680, 0.6598, 0.4025], [ 0.3693, 0.4834, 0.2048, 0.0112, 0.3305, 0.2773, 0.4418, 0.3806, 0.6904, 0.4129], [ 0.5209, 0.6908, 0.2949, 0.0601, 0.3974, 0.3924, 0.4738, 0.5311, 0.7260, 0.3913], [ 0.3983, 0.4815, 0.3269, -0.0498, 0.3313, 0.3176, 0.5540, 0.4679, 0.6727, 0.4875], [ 0.2548, 0.4201, 0.1639, 0.0116, 0.2426, 0.2281, 0.4484, 0.4445, 0.4960, 0.3393], [ 0.4534, 0.5900, 0.2193, -0.0076, 0.3166, 0.3599, 0.4884, 0.4361, 0.7055, 0.4458], [ 0.3559, 0.5106, 0.2420, 0.0457, 0.3540, 0.2792, 0.4954, 0.4229, 0.6938, 0.4276]], grad_fn=&lt;AddBackward0&gt;) These are our r2 outputs: tensor([[0.1788, 0.3784, 0.1961, 0.0859, 0.2355, 0.1763, 0.2745, 0.2243, 0.3367, 0.2174], [0.3220, 0.4116, 0.2579, 0.0000, 0.2618, 0.2804, 0.4132, 0.4018, 0.5158, 0.3829], [0.4339, 0.6410, 0.3890, 0.0903, 0.4595, 0.3141, 0.4068, 0.4590, 0.6507, 0.3687], [0.3566, 0.5638, 0.3251, 0.0046, 0.3596, 0.3282, 0.4666, 0.3938, 0.6289, 0.4177], [0.2960, 0.4863, 0.3065, 0.0000, 0.1770, 0.3261, 0.4764, 0.4248, 0.3754, 0.3667], [0.3670, 0.4384, 0.2241, 0.0423, 0.3642, 0.2121, 0.3428, 0.5588, 0.5101, 0.2422], [0.2763, 0.3741, 0.1834, 0.0262, 0.2649, 0.2036, 0.3613, 0.4662, 0.4422, 0.2451], ..., [0.3498, 0.4886, 0.2095, 0.0291, 0.3451, 0.2555, 0.4172, 0.3680, 0.6598, 0.4025], [0.3693, 0.4834, 0.2048, 0.0112, 0.3305, 0.2773, 0.4418, 0.3806, 0.6904, 0.4129], [0.5209, 0.6908, 0.2949, 0.0601, 0.3974, 0.3924, 0.4738, 0.5311, 0.7260, 0.3913], [0.3983, 0.4815, 0.3269, 0.0000, 0.3313, 0.3176, 0.5540, 0.4679, 0.6727, 0.4875], [0.2548, 0.4201, 0.1639, 0.0116, 0.2426, 0.2281, 0.4484, 0.4445, 0.4960, 0.3393], [0.4534, 0.5900, 0.2193, 0.0000, 0.3166, 0.3599, 0.4884, 0.4361, 0.7055, 0.4458], [0.3559, 0.5106, 0.2420, 0.0457, 0.3540, 0.2792, 0.4954, 0.4229, 0.6938, 0.4276]], grad_fn=&lt;ClampBackward1&gt;) These are our l3 outputs: tensor([[1.7382, 1.0127], [2.3116, 1.5027], [3.1483, 1.8254], [2.8914, 1.7114], [2.1701, 1.6438], [2.2242, 1.4845], [1.9086, 1.3069], ..., [2.7114, 1.5372], [2.7652, 1.5826], [3.3515, 2.0682], [2.8961, 1.8331], [2.1227, 1.4342], [3.0565, 1.8808], [2.8763, 1.6804]], grad_fn=&lt;AddBackward0&gt;) These are our softmax outputs: tensor([[0.6738, 0.3262], [0.6919, 0.3081], [0.7897, 0.2103], [0.7649, 0.2351], [0.6286, 0.3714], [0.6769, 0.3231], [0.6460, 0.3540], ..., [0.7639, 0.2361], [0.7654, 0.2346], [0.7830, 0.2170], [0.7433, 0.2567], [0.6656, 0.3344], [0.7642, 0.2358], [0.7678, 0.2322]], grad_fn=&lt;DivBackward0&gt;) . . y_preds = softmax.output . Defining our loss function: . Negative log loss is going to be our loss function. We use our predictions from softmax to calculate the loss. I won&#39;t bore youwith all the explanations. That is done in the resources I mentioned above, in a better way than I ever can. . class negative_log_loss: def calculate(self, y_preds, y_true): samples = len(y_preds) y_pred_clipped = torch.clip(y_preds, 1e-7, 1-1e-7) if len(y_true.shape) == 1: correct_confidences = y_pred_clipped[torch.tensor(range(samples)), y_true] elif len(y_true.shape) == 2: correct_confidences = torch.sum(y_pred_clipped*y_true, axis=1) negative_log_likelihoods = -torch.log(correct_confidences) return torch.mean(negative_log_likelihoods) . &#39;Training&#39; our neural network . First we are gonna write a function to update our weights and biases according to our loss(i.e. by reducing the product of the gradient and learning rate by our w&amp;bs.) . def update_wandbs(wandbs, lr): for layer in wandbs: layer.sub_(layer.grad * lr) #print(layer.grad) layer.grad.zero_() . Then we write a training loop for our network, to train for one &#39;epoch&#39;. . def one_epoch(wandbs, lr, inputs): layer1.weights, layer2.weights, layer3.weights, layer1.biases, layer2.biases, layer3.biases = wandbs #print(&#39;These are our weights and biases:&#39;) #print(wandbs) layer1.forward(inputs) #print(&quot; n&quot;) #print(&#39;These are our l1 outputs:&#39;) #print(layer1.output) relu1.forward(layer1.output) #print(&quot; n&quot;) #print(&#39;These are our r1 outputs:&#39;) #print(relu1.output) layer2.forward(relu1.output) #print(&quot; n&quot;) #print(&#39;These are our l2 outputs:&#39;) #print(layer2.output) relu2.forward(layer2.output) #print(&quot; n&quot;) #print(&#39;These are our r2 outputs:&#39;) #print(relu2.output) layer3.forward(relu2.output) #print(&quot; n&quot;) #print(&#39;These are our l3 outputs:&#39;) #print(layer3.output) softmax = Softmax_act() softmax.forward(layer3.output) y_preds = softmax.output #print(&quot; n&quot;) #print(&#39;These are our softmax outputs:&#39;) #print(softmax.output) nll = negative_log_loss() loss = nll.calculate(y_preds, y_true) loss.backward() with torch.no_grad(): update_wandbs(wandbs, 2) print(f&quot;{loss:.3f}&quot;, end=&quot;; &quot;) . Then, we define another function so that we can train the model easily for multiple epochs. Learning rate is an important hyperparameter here. Refer to Jeremy&#39;s notebooks/videos if you don&#39;t already know about it. . def train_model(wandbs, lr, inputs, epochs=30): #torch.manual_seed(442) for i in range(epochs): one_epoch(wandbs, lr, inputs) return y_preds . y_preds = train_model(wandbs, 0.5, inputs, epochs=40) . 0.801; 1.816; 0.827; 0.694; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; 0.693; . y_preds . tensor([[0.6738, 0.3262], [0.6919, 0.3081], [0.7897, 0.2103], [0.7649, 0.2351], [0.6286, 0.3714], [0.6769, 0.3231], [0.6460, 0.3540], ..., [0.7639, 0.2361], [0.7654, 0.2346], [0.7830, 0.2170], [0.7433, 0.2567], [0.6656, 0.3344], [0.7642, 0.2358], [0.7678, 0.2322]], grad_fn=&lt;DivBackward0&gt;) . def accuracy(y_preds, y_true): samples = len(y_preds) return print(f&#39;Accuracy is {(y_true.bool()==(y_preds[torch.tensor(range(samples)), y_true]&gt;0.5)).float().mean()*100 :.3f} percent&#39;) . accuracy(y_preds, y_true) . Accuracy is 0.000 percent . Well, My network is still pretty crappy. I tried a lot, but couldn&#39;t get it to train yet. I&#39;m gonna keep trying. But for now, I&#39;m going to use a framework to make my life easier. Afterall, purpose of this whole exercise was not to get an accurate model, but to understand the nuts and bolts of a neural network! .",
            "url": "https://truthdead.github.io/nival-blog/2022/08/30/neural-nets.html",
            "relUrl": "/2022/08/30/neural-nets.html",
            "date": " • Aug 30, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "import matplotlib.pyplot as plt import pandas as pd import numpy as np %matplotlib inline import seaborn as sns . thal = pd.read_csv(&#39;thalcleaned_outliers.csv&#39;) thal2 = pd.read_csv(&#39;thalcleaned_outliers2.csv&#39;) . thal.describe() . no hb pcv rbc mcv mch mchc rdw wbc neut lymph plt hba hba2 hbf . count 243.000000 | 243.000000 | 243.000000 | 242.000000 | 243.000000 | 241.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | . mean 143.514403 | 11.799712 | 36.109301 | 5.086123 | 72.251140 | 23.452697 | 32.209671 | 15.208232 | 101.878340 | 46.986866 | 42.840218 | 329.805780 | 86.098491 | 2.751195 | 0.664832 | . std 83.962622 | 1.864599 | 4.979802 | 0.651812 | 9.785879 | 3.982755 | 2.123092 | 2.498933 | 1343.898711 | 12.194046 | 12.057044 | 122.601262 | 4.736257 | 0.712732 | 0.703808 | . min 0.000000 | 6.000000 | 19.870000 | 2.410000 | 47.700000 | 11.100000 | 19.400000 | 10.800000 | 2.300000 | 6.200000 | 10.300000 | 8.000000 | 25.000000 | 0.300000 | 0.000000 | . 25% 72.000000 | 10.650000 | 33.110000 | 4.690000 | 65.400000 | 20.700000 | 31.200000 | 13.500000 | 7.375000 | 43.989189 | 36.135000 | 258.000000 | 84.864586 | 2.500000 | 0.300000 | . 50% 143.000000 | 11.700000 | 35.876404 | 5.080000 | 70.500000 | 23.000000 | 32.300000 | 14.827660 | 8.900000 | 47.565000 | 41.537931 | 309.000000 | 86.523291 | 2.600000 | 0.537931 | . 75% 217.500000 | 13.000000 | 38.900000 | 5.500000 | 80.450000 | 26.400000 | 33.400000 | 16.300000 | 10.450000 | 54.600000 | 45.527027 | 386.000000 | 87.331429 | 2.750000 | 0.769231 | . max 287.000000 | 16.700000 | 51.100000 | 7.470000 | 91.700000 | 35.600000 | 40.800000 | 28.800000 | 20900.000000 | 77.500000 | 87.000000 | 1107.000000 | 98.400000 | 5.800000 | 5.800000 | . thal2.describe() . Unnamed: 0 hb pcv rbc mcv mch mchc rdw wbc neut lymph plt hba hba2 hbf . count 243.000000 | 243.000000 | 243.000000 | 242.000000 | 243.000000 | 242.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | . mean 121.000000 | 11.799712 | 36.109301 | 5.086123 | 72.251140 | 23.446438 | 32.209671 | 15.208232 | 101.878340 | 46.986866 | 42.840218 | 329.805780 | 86.098491 | 2.751195 | 0.664832 | . std 70.292247 | 1.864599 | 4.979802 | 0.651812 | 9.785879 | 3.975676 | 2.123092 | 2.498933 | 1343.898711 | 12.194046 | 12.057044 | 122.601262 | 4.736257 | 0.712732 | 0.703808 | . min 0.000000 | 6.000000 | 19.870000 | 2.410000 | 47.700000 | 11.100000 | 19.400000 | 10.800000 | 2.300000 | 6.200000 | 10.300000 | 8.000000 | 25.000000 | 0.300000 | 0.000000 | . 25% 60.500000 | 10.650000 | 33.110000 | 4.690000 | 65.400000 | 20.725000 | 31.200000 | 13.500000 | 7.375000 | 43.989189 | 36.135000 | 258.000000 | 84.864586 | 2.500000 | 0.300000 | . 50% 121.000000 | 11.700000 | 35.876404 | 5.080000 | 70.500000 | 23.000000 | 32.300000 | 14.827660 | 8.900000 | 47.565000 | 41.537931 | 309.000000 | 86.523291 | 2.600000 | 0.537931 | . 75% 181.500000 | 13.000000 | 38.900000 | 5.500000 | 80.450000 | 26.400000 | 33.400000 | 16.300000 | 10.450000 | 54.600000 | 45.527027 | 386.000000 | 87.331429 | 2.750000 | 0.769231 | . max 242.000000 | 16.700000 | 51.100000 | 7.470000 | 91.700000 | 35.600000 | 40.800000 | 28.800000 | 20900.000000 | 77.500000 | 87.000000 | 1107.000000 | 98.400000 | 5.800000 | 5.800000 | . thal.drop(columns=&#39;no&#39;, inplace=True) . thal.describe() . hb pcv rbc mcv mch mchc rdw wbc neut lymph plt hba hba2 hbf . count 243.000000 | 243.000000 | 242.000000 | 243.000000 | 241.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | 243.000000 | . mean 11.799712 | 36.109301 | 5.086123 | 72.251140 | 23.452697 | 32.209671 | 15.208232 | 101.878340 | 46.986866 | 42.840218 | 329.805780 | 86.098491 | 2.751195 | 0.664832 | . std 1.864599 | 4.979802 | 0.651812 | 9.785879 | 3.982755 | 2.123092 | 2.498933 | 1343.898711 | 12.194046 | 12.057044 | 122.601262 | 4.736257 | 0.712732 | 0.703808 | . min 6.000000 | 19.870000 | 2.410000 | 47.700000 | 11.100000 | 19.400000 | 10.800000 | 2.300000 | 6.200000 | 10.300000 | 8.000000 | 25.000000 | 0.300000 | 0.000000 | . 25% 10.650000 | 33.110000 | 4.690000 | 65.400000 | 20.700000 | 31.200000 | 13.500000 | 7.375000 | 43.989189 | 36.135000 | 258.000000 | 84.864586 | 2.500000 | 0.300000 | . 50% 11.700000 | 35.876404 | 5.080000 | 70.500000 | 23.000000 | 32.300000 | 14.827660 | 8.900000 | 47.565000 | 41.537931 | 309.000000 | 86.523291 | 2.600000 | 0.537931 | . 75% 13.000000 | 38.900000 | 5.500000 | 80.450000 | 26.400000 | 33.400000 | 16.300000 | 10.450000 | 54.600000 | 45.527027 | 386.000000 | 87.331429 | 2.750000 | 0.769231 | . max 16.700000 | 51.100000 | 7.470000 | 91.700000 | 35.600000 | 40.800000 | 28.800000 | 20900.000000 | 77.500000 | 87.000000 | 1107.000000 | 98.400000 | 5.800000 | 5.800000 | . thal.drop? . thal.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 243 entries, 0 to 242 Data columns (total 16 columns): # Column Non-Null Count Dtype -- -- 0 sex 243 non-null object 1 hb 243 non-null float64 2 pcv 243 non-null float64 3 rbc 242 non-null float64 4 mcv 243 non-null float64 5 mch 241 non-null float64 6 mchc 243 non-null float64 7 rdw 243 non-null float64 8 wbc 243 non-null float64 9 neut 243 non-null float64 10 lymph 243 non-null float64 11 plt 243 non-null float64 12 hba 243 non-null float64 13 hba2 243 non-null float64 14 hbf 243 non-null float64 15 phenotype 243 non-null object dtypes: float64(14), object(2) memory usage: 30.5+ KB . thal.astype({&#39;phenotype&#39; : &#39;category&#39;, &#39;sex&#39; : &#39;category&#39;}) . sex hb pcv rbc mcv mch mchc rdw wbc neut lymph plt hba hba2 hbf phenotype . 0 female | 10.8 | 35.2 | 5.12 | 68.7 | 21.2 | 30.8 | 13.4 | 9.6 | 53.000000 | 33.000000 | 309.0 | 88.500000 | 2.600000 | 0.110000 | alpha trait | . 1 male | 10.8 | 26.6 | 4.28 | 62.1 | 25.3 | 40.8 | 19.8 | 10.3 | 49.400000 | 43.100000 | 687.0 | 87.800000 | 2.400000 | 0.900000 | alpha trait | . 2 female | 10.8 | 35.2 | 5.12 | 68.7 | 21.2 | 30.8 | 13.4 | 9.6 | 53.000000 | 33.000000 | 309.0 | 88.500000 | 2.600000 | 0.100000 | silent carrier | . 3 male | 14.5 | 43.5 | 5.17 | 84.0 | 28.0 | 33.4 | 12.1 | 11.9 | 31.000000 | 50.000000 | 334.0 | 86.800000 | 2.800000 | 0.300000 | silent carrier | . 4 male | 11.5 | 34.4 | 5.02 | 68.7 | 22.9 | 33.4 | 15.7 | 20.4 | 67.000000 | 30.000000 | 596.0 | 86.300000 | 2.400000 | 1.300000 | silent carrier | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 238 male | 15.5 | 45.9 | 5.19 | 88.4 | 29.9 | 33.8 | 12.6 | 8.8 | 47.565000 | 40.975000 | 177.0 | 88.600000 | 3.200000 | 0.400000 | normal | . 239 female | 10.4 | 33.3 | 4.93 | 67.6 | 21.1 | 31.2 | 14.8 | 8.9 | 44.478378 | 45.527027 | 295.0 | 88.000000 | 2.400000 | 0.500000 | silent carrier | . 240 male | 9.8 | 29.8 | 4.75 | 62.7 | 19.0 | 30.4 | 14.7 | 7.2 | 48.234483 | 41.537931 | 262.0 | 85.100000 | 2.400000 | 1.100000 | alpha trait | . 241 male | 11.2 | 37.2 | 5.43 | 68.5 | 20.6 | 30.1 | 15.1 | 12.0 | 13.500000 | 76.800000 | 277.0 | 86.523291 | 2.588608 | 0.769231 | silent carrier | . 242 male | 14.4 | 44.5 | 5.70 | 78.0 | 25.3 | 31.2 | 15.0 | 7.2 | 36.000000 | 59.000000 | 224.0 | 86.523291 | 2.588608 | 0.769231 | silent carrier | . 243 rows × 16 columns . thal[&#39;rbc&#39;] = thal[&#39;rbc&#39;].fillna(thal.groupby(&#39;phenotype&#39;)[&#39;rbc&#39;].transform(&#39;mean&#39;)) thal[&#39;mch&#39;] = thal[&#39;mch&#39;].fillna(thal.groupby(&#39;phenotype&#39;)[&#39;mch&#39;].transform(&#39;mean&#39;)) . thal.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 243 entries, 0 to 242 Data columns (total 16 columns): # Column Non-Null Count Dtype -- -- 0 sex 243 non-null object 1 hb 243 non-null float64 2 pcv 243 non-null float64 3 rbc 243 non-null float64 4 mcv 243 non-null float64 5 mch 243 non-null float64 6 mchc 243 non-null float64 7 rdw 243 non-null float64 8 wbc 243 non-null float64 9 neut 243 non-null float64 10 lymph 243 non-null float64 11 plt 243 non-null float64 12 hba 243 non-null float64 13 hba2 243 non-null float64 14 hbf 243 non-null float64 15 phenotype 243 non-null object dtypes: float64(14), object(2) memory usage: 30.5+ KB . thal = thal.astype({&quot;phenotype&quot; : &quot;category&quot;, &quot;sex&quot; : &quot;category&quot;}) . thal.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 243 entries, 0 to 242 Data columns (total 16 columns): # Column Non-Null Count Dtype -- -- 0 sex 243 non-null category 1 hb 243 non-null float64 2 pcv 243 non-null float64 3 rbc 243 non-null float64 4 mcv 243 non-null float64 5 mch 243 non-null float64 6 mchc 243 non-null float64 7 rdw 243 non-null float64 8 wbc 243 non-null float64 9 neut 243 non-null float64 10 lymph 243 non-null float64 11 plt 243 non-null float64 12 hba 243 non-null float64 13 hba2 243 non-null float64 14 hbf 243 non-null float64 15 phenotype 243 non-null category dtypes: category(2), float64(14) memory usage: 27.5 KB . 243 out 288 was obtained - after removing elements where there was no diagnosis. | . thal.sex . 0 female 1 male 2 female 3 male 4 male ... 238 male 239 female 240 male 241 male 242 male Name: sex, Length: 243, dtype: category Categories (2, object): [&#39;female&#39;, &#39;male&#39;] . thal.loc[&#39;sex&#39; == &#39;male&#39;] #this is WRONG . KeyError Traceback (most recent call last) &lt;ipython-input-15-f64bf44e4e9a&gt; in &lt;module&gt; 1 #calculating the male percentage -&gt; 2 thal.loc[&#39;sex&#39; == &#39;male&#39;] #this is WRONG ~ Anaconda3 lib site-packages pandas core indexing.py in __getitem__(self, key) 893 894 maybe_callable = com.apply_if_callable(key, self.obj) --&gt; 895 return self._getitem_axis(maybe_callable, axis=axis) 896 897 def _is_scalar_access(self, key: Tuple): ~ Anaconda3 lib site-packages pandas core indexing.py in _getitem_axis(self, key, axis) 1122 # fall thru to straight lookup 1123 self._validate_key(key, axis) -&gt; 1124 return self._get_label(key, axis=axis) 1125 1126 def _get_slice_axis(self, slice_obj: slice, axis: int): ~ Anaconda3 lib site-packages pandas core indexing.py in _get_label(self, label, axis) 1071 def _get_label(self, label, axis: int): 1072 # GH#5667 this will fail if the label is not present in the axis. -&gt; 1073 return self.obj.xs(label, axis=axis) 1074 1075 def _handle_lowerdim_multi_index_axis0(self, tup: Tuple): ~ Anaconda3 lib site-packages pandas core generic.py in xs(self, key, axis, level, drop_level) 3737 raise TypeError(f&#34;Expected label or tuple of labels, got {key}&#34;) from e 3738 else: -&gt; 3739 loc = index.get_loc(key) 3740 3741 if isinstance(loc, np.ndarray): ~ Anaconda3 lib site-packages pandas core indexes range.py in get_loc(self, key, method, tolerance) 352 except ValueError as err: 353 raise KeyError(key) from err --&gt; 354 raise KeyError(key) 355 return super().get_loc(key, method=method, tolerance=tolerance) 356 KeyError: False . SEE THIS LINK TO KNOW WHY ABOVE IS WRONG - https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html . female_pct = (thal[&#39;sex&#39;] == &#39;female&#39;).mean()*100 female_pct . 45.67901234567901 . thal.age.mean() . AttributeError Traceback (most recent call last) &lt;ipython-input-17-9ca328a0bd78&gt; in &lt;module&gt; -&gt; 1 thal.age.mean() ~ Anaconda3 lib site-packages pandas core generic.py in __getattr__(self, name) 5463 if self._info_axis._can_hold_identifiers_and_holds_name(name): 5464 return self[name] -&gt; 5465 return object.__getattribute__(self, name) 5466 5467 def __setattr__(self, name: str, value) -&gt; None: AttributeError: &#39;DataFrame&#39; object has no attribute &#39;age&#39; . d_list = list(thal.phenotype.unique()) d_list . [&#39;alpha trait&#39;, &#39;silent carrier&#39;, &#39;normal&#39;, &#39;iron deficiency&#39;, &#39;beta trait&#39;] . thal.phenotype.value_counts() . silent carrier 105 normal 58 alpha trait 42 iron deficiency 21 beta trait 17 Name: phenotype, dtype: int64 . plt.style.use(&#39;ggplot&#39;) plt.rcParams[&#39;font.size&#39;] = &#39;18&#39; plt.pie(thal.phenotype.value_counts(),counterclock=False, startangle=90, autopct = &#39;%.2f%%&#39;) #plt.figure(figsize=(16, 10))from pylab import rcParams from pylab import rcParams rcParams[&#39;figure.figsize&#39;] = 16,10 plt.legend(labels=[&#39;Alpha trait&#39;, &#39;Silent carrier&#39;, &#39;Normal&#39;, &#39;Iron deficiency&#39;, &#39;Beta trait&#39;]) . &lt;matplotlib.legend.Legend at 0x28169e47488&gt; . plt.style.use(&#39;ggplot&#39;) plt.rcParams[&#39;font.size&#39;] = &#39;18&#39; plt.pie(alphanorm.phenotype.value_counts(),counterclock=False, startangle=90, autopct = &#39;%.2f%%&#39;) #plt.figure(figsize=(16, 10))from pylab import rcParams from pylab import rcParams rcParams[&#39;figure.figsize&#39;] = 16,10 plt.legend(labels=[&#39;Alpha-thalassemia carriers&#39;, &#39;Normal&#39;]) . &lt;matplotlib.legend.Legend at 0x2816a097148&gt; . female_pct = (alphanorm[&#39;sex&#39;] == &#39;female&#39;).mean()*100 female_pct . 44.827586206896555 . Final dataset, after removing samples without a specific diagnosis, consisted of 243 samples. Basic demographics are depicted in the Figure 1. Males represented just over half of the dataset, and the mean age was...... . Model 01 Analysis - AlphaThal vs Normal . . alphanorm = pd.read_csv(&#39;alphanorm.csv&#39;, index_col = False) . alphanorm.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 203 entries, 0 to 202 Data columns (total 16 columns): # Column Non-Null Count Dtype -- -- 0 sex 203 non-null object 1 hb 203 non-null float64 2 pcv 203 non-null float64 3 rbc 202 non-null float64 4 mcv 203 non-null float64 5 mch 201 non-null float64 6 mchc 203 non-null float64 7 rdw 203 non-null float64 8 wbc 203 non-null float64 9 neut 203 non-null float64 10 lymph 203 non-null float64 11 plt 203 non-null float64 12 hba 203 non-null float64 13 hba2 203 non-null float64 14 hbf 203 non-null float64 15 phenotype 203 non-null object dtypes: float64(14), object(2) memory usage: 25.5+ KB . alphanorm.describe() . hb pcv rbc mcv mch mchc rdw wbc neut lymph plt hba hba2 hbf . count 203.000000 | 203.000000 | 202.000000 | 203.000000 | 201.000000 | 203.000000 | 203.000000 | 203.000000 | 203.00000 | 203.000000 | 203.000000 | 203.000000 | 203.000000 | 203.000000 | . mean 12.111823 | 36.676757 | 5.057432 | 74.167128 | 24.200498 | 32.497322 | 14.848380 | 9.188423 | 46.08398 | 43.516162 | 328.265663 | 86.515666 | 2.579554 | 0.664367 | . std 1.757800 | 4.821295 | 0.585929 | 9.280344 | 3.805844 | 1.979277 | 2.381027 | 2.591782 | 11.79773 | 11.992417 | 114.284337 | 2.436432 | 0.312889 | 0.739903 | . min 7.600000 | 22.100000 | 2.410000 | 47.700000 | 11.100000 | 21.100000 | 10.800000 | 2.300000 | 6.20000 | 10.300000 | 100.000000 | 68.000000 | 0.300000 | 0.000000 | . 25% 10.900000 | 33.300000 | 4.700000 | 66.950000 | 21.200000 | 31.550000 | 13.300000 | 7.500000 | 43.00000 | 37.000000 | 256.000000 | 85.200000 | 2.500000 | 0.300000 | . 50% 12.000000 | 36.000000 | 5.025000 | 73.800000 | 24.100000 | 32.500000 | 14.700000 | 8.915278 | 47.56500 | 41.537931 | 310.000000 | 86.523291 | 2.600000 | 0.537931 | . 75% 13.350000 | 39.150000 | 5.437500 | 81.900000 | 26.800000 | 33.446296 | 15.950000 | 10.550000 | 52.15000 | 46.000000 | 379.500000 | 87.365714 | 2.700000 | 0.769231 | . max 16.700000 | 51.100000 | 6.770000 | 91.700000 | 35.600000 | 40.800000 | 28.800000 | 20.400000 | 77.50000 | 87.000000 | 1107.000000 | 98.400000 | 3.300000 | 5.800000 | . alphanorm.head() . sex hb pcv rbc mcv mch mchc rdw wbc neut lymph plt hba hba2 hbf phenotype . 0 female | 10.8 | 35.2 | 5.12 | 68.7 | 21.2 | 30.8 | 13.4 | 9.6 | 53.0 | 33.0 | 309.0 | 88.5 | 2.6 | 0.11 | alpha carrier | . 1 male | 10.8 | 26.6 | 4.28 | 62.1 | 25.3 | 40.8 | 19.8 | 10.3 | 49.4 | 43.1 | 687.0 | 87.8 | 2.4 | 0.90 | alpha carrier | . 2 female | 10.8 | 35.2 | 5.12 | 68.7 | 21.2 | 30.8 | 13.4 | 9.6 | 53.0 | 33.0 | 309.0 | 88.5 | 2.6 | 0.10 | alpha carrier | . 3 male | 14.5 | 43.5 | 5.17 | 84.0 | 28.0 | 33.4 | 12.1 | 11.9 | 31.0 | 50.0 | 334.0 | 86.8 | 2.8 | 0.30 | alpha carrier | . 4 male | 11.5 | 34.4 | 5.02 | 68.7 | 22.9 | 33.4 | 15.7 | 20.4 | 67.0 | 30.0 | 596.0 | 86.3 | 2.4 | 1.30 | alpha carrier | . only 203 when considering alpha carries and normals | . alphanorm[&#39;rbc&#39;] = alphanorm[&#39;rbc&#39;].fillna(alphanorm.groupby(&#39;phenotype&#39;)[&#39;rbc&#39;].transform(&#39;mean&#39;)) alphanorm[&#39;mch&#39;] = alphanorm[&#39;mch&#39;].fillna(alphanorm.groupby(&#39;phenotype&#39;)[&#39;mch&#39;].transform(&#39;mean&#39;)) . alphanorm.phenotype.value_counts() . alpha carrier 148 normal 55 Name: phenotype, dtype: int64 . alphanorm.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 203 entries, 0 to 202 Data columns (total 16 columns): # Column Non-Null Count Dtype -- -- 0 sex 203 non-null object 1 hb 203 non-null float64 2 pcv 203 non-null float64 3 rbc 203 non-null float64 4 mcv 203 non-null float64 5 mch 203 non-null float64 6 mchc 203 non-null float64 7 rdw 203 non-null float64 8 wbc 203 non-null float64 9 neut 203 non-null float64 10 lymph 203 non-null float64 11 plt 203 non-null float64 12 hba 203 non-null float64 13 hba2 203 non-null float64 14 hbf 203 non-null float64 15 phenotype 203 non-null object dtypes: float64(14), object(2) memory usage: 25.5+ KB . Changing fonts in each elements of a plot : https://stackoverflow.com/questions/12444716/how-do-i-set-the-figure-title-and-axes-labels-font-size-in-matplotlib . #will include these variables as well alphanorm = alphanorm.drop([&#39;hbf&#39;, &#39;wbc&#39;, &#39;neut&#39;, &#39;lymph&#39;], axis=1) . alphanorm.dtypes . sex object hb float64 pcv float64 rbc float64 mcv float64 mch float64 mchc float64 rdw float64 plt float64 hba float64 hba2 float64 phenotype object dtype: object . alphanorm.describe() . hb pcv rbc mcv mch mchc rdw plt hba hba2 . count 203.000000 | 203.000000 | 203.000000 | 203.000000 | 203.000000 | 203.000000 | 203.000000 | 203.000000 | 203.000000 | 203.000000 | . mean 12.111823 | 36.676757 | 5.057293 | 74.167128 | 24.202443 | 32.497322 | 14.848380 | 328.265663 | 86.515666 | 2.579554 | . std 1.757800 | 4.821295 | 0.584480 | 9.280344 | 3.787007 | 1.979277 | 2.381027 | 114.284337 | 2.436432 | 0.312889 | . min 7.600000 | 22.100000 | 2.410000 | 47.700000 | 11.100000 | 21.100000 | 10.800000 | 100.000000 | 68.000000 | 0.300000 | . 25% 10.900000 | 33.300000 | 4.700000 | 66.950000 | 21.250000 | 31.550000 | 13.300000 | 256.000000 | 85.200000 | 2.500000 | . 50% 12.000000 | 36.000000 | 5.029125 | 73.800000 | 24.200000 | 32.500000 | 14.700000 | 310.000000 | 86.523291 | 2.600000 | . 75% 13.350000 | 39.150000 | 5.435000 | 81.900000 | 26.800000 | 33.446296 | 15.950000 | 379.500000 | 87.365714 | 2.700000 | . max 16.700000 | 51.100000 | 6.770000 | 91.700000 | 35.600000 | 40.800000 | 28.800000 | 1107.000000 | 98.400000 | 3.300000 | . for col in alphanorm.columns: if alphanorm[col].dtype != object: Q1 = alphanorm[col].quantile(0.25) Q3 = alphanorm[col].quantile(0.75) IQR = Q3 - Q1 S = 1.5*IQR LB = Q1 - S UB = Q3 + S print(UB) alphanorm.loc[alphanorm[col] &gt; UB,col] = UB alphanorm.loc[alphanorm[col] &lt; LB,col] = LB . 17.025000000000002 47.92500000000002 6.537500000000001 104.32500000000003 35.125 36.29074075 19.924999999999997 564.75 90.61428571249999 3.0000000000000004 . alphanorm.describe() . hb pcv rbc mcv mch mchc rdw plt hba hba2 . count 203.000000 | 203.000000 | 203.000000 | 203.000000 | 203.000000 | 203.000000 | 203.000000 | 203.000000 | 203.000000 | 203.000000 | . mean 12.111823 | 36.668013 | 5.061197 | 74.167128 | 24.209093 | 32.541985 | 14.739390 | 323.875269 | 86.457172 | 2.600737 | . std 1.757800 | 4.730102 | 0.557916 | 9.280344 | 3.750817 | 1.616079 | 1.971135 | 96.533639 | 1.523440 | 0.197139 | . min 7.600000 | 24.525000 | 3.597500 | 47.700000 | 12.925000 | 28.705556 | 10.800000 | 100.000000 | 81.951429 | 2.200000 | . 25% 10.900000 | 33.300000 | 4.700000 | 66.950000 | 21.250000 | 31.550000 | 13.300000 | 256.000000 | 85.200000 | 2.500000 | . 50% 12.000000 | 36.000000 | 5.029125 | 73.800000 | 24.200000 | 32.500000 | 14.700000 | 310.000000 | 86.523291 | 2.600000 | . 75% 13.350000 | 39.150000 | 5.435000 | 81.900000 | 26.800000 | 33.446296 | 15.950000 | 379.500000 | 87.365714 | 2.700000 | . max 16.700000 | 47.925000 | 6.537500 | 91.700000 | 35.125000 | 36.290741 | 19.925000 | 564.750000 | 90.614286 | 3.000000 | . alphanorm.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 203 entries, 0 to 202 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 sex 203 non-null object 1 hb 203 non-null float64 2 pcv 203 non-null float64 3 rbc 203 non-null float64 4 mcv 203 non-null float64 5 mch 203 non-null float64 6 mchc 203 non-null float64 7 rdw 203 non-null float64 8 plt 203 non-null float64 9 hba 203 non-null float64 10 hba2 203 non-null float64 11 phenotype 203 non-null object dtypes: float64(10), object(2) memory usage: 19.2+ KB . alphanorm = alphanorm.astype({&#39;sex&#39; : &#39;category&#39;, &#39;phenotype&#39; : &#39;category&#39;}) . alphanorm.phenotype.value_counts() . alpha carrier 148 normal 55 Name: phenotype, dtype: int64 . . alphanorm[&#39;phenotype&#39;] = alphanorm[&#39;phenotype&#39;] == &#39;alpha carrier&#39; alphanorm[&#39;phenotype&#39;] = alphanorm[&#39;phenotype&#39;].replace({True:1, False:0}) . alphanorm.head(200) . sex hb pcv rbc mcv mch mchc rdw plt hba hba2 phenotype . 0 female | 10.8 | 35.2 | 5.12 | 68.7 | 21.2 | 30.800000 | 13.4 | 309.00 | 88.5 | 2.6 | 1 | . 1 male | 10.8 | 26.6 | 4.28 | 62.1 | 25.3 | 36.290741 | 19.8 | 564.75 | 87.8 | 2.4 | 1 | . 2 female | 10.8 | 35.2 | 5.12 | 68.7 | 21.2 | 30.800000 | 13.4 | 309.00 | 88.5 | 2.6 | 1 | . 3 male | 14.5 | 43.5 | 5.17 | 84.0 | 28.0 | 33.400000 | 12.1 | 334.00 | 86.8 | 2.8 | 1 | . 4 male | 11.5 | 34.4 | 5.02 | 68.7 | 22.9 | 33.400000 | 15.7 | 564.75 | 86.3 | 2.4 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 195 female | 11.4 | 36.5 | 5.23 | 69.8 | 21.8 | 31.300000 | 15.9 | 528.00 | 85.7 | 2.7 | 0 | . 196 female | 13.1 | 39.9 | 4.88 | 81.8 | 26.9 | 32.800000 | 15.6 | 268.00 | 86.7 | 2.7 | 0 | . 197 male | 15.3 | 46.9 | 5.20 | 90.2 | 29.5 | 32.700000 | 12.7 | 215.00 | 86.7 | 2.6 | 0 | . 198 male | 15.5 | 45.9 | 5.19 | 88.4 | 29.9 | 33.800000 | 12.6 | 177.00 | 88.6 | 3.0 | 0 | . 199 female | 10.4 | 33.3 | 4.93 | 67.6 | 21.1 | 31.200000 | 14.8 | 295.00 | 88.0 | 2.4 | 0 | . 200 rows × 12 columns . Converting boolean column to int : https://stackoverflow.com/questions/17383094/how-can-i-map-true-false-to-1-0-in-a-pandas-dataframe . X = alphanorm.drop(&#39;phenotype&#39;, axis=1) y = alphanorm[&#39;phenotype&#39;] . y.value_counts() . 1 148 0 55 Name: phenotype, dtype: int64 . categorical_vars = list((X.select_dtypes(include=[&#39;category&#39;])).columns) categorical_vars . [&#39;sex&#39;] . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer one_hot = OneHotEncoder() transformer = ColumnTransformer([(&#39;one_hot&#39;, one_hot, categorical_vars)], remainder= &#39;passthrough&#39;) #transformed_alpha = transformer.fit_transform(alphanorm) #transformed_alpha . df_transformed = pd.DataFrame(transformed_alpha) df_transformed.head() . NameError Traceback (most recent call last) &lt;ipython-input-24-82979c2eac64&gt; in &lt;module&gt; -&gt; 1 df_transformed = pd.DataFrame(transformed_alpha) 2 df_transformed.head() NameError: name &#39;transformed_alpha&#39; is not defined . from sklearn.model_selection import StratifiedShuffleSplit split = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=42) for train_index, test_index in split.split(alphanorm, alphanorm[&quot;phenotype&quot;]): strat_train = alphanorm.loc[train_index] strat_test = alphanorm.loc[test_index] . strat_train.head(100) . sex hb pcv rbc mcv mch mchc rdw plt hba hba2 phenotype . 181 female | 11.3 | 35.900000 | 4.71 | 76.0 | 24.0 | 31.5 | 14.0 | 268.00000 | 86.523291 | 2.588608 | 0 | . 171 female | 11.6 | 36.000000 | 4.69 | 77.0 | 25.0 | 32.2 | 13.3 | 249.00000 | 86.600000 | 2.500000 | 0 | . 95 female | 13.8 | 35.876404 | 5.81 | 74.0 | 23.7 | 31.6 | 11.8 | 312.00000 | 87.100000 | 2.600000 | 1 | . 41 male | 11.8 | 36.000000 | 5.27 | 68.3 | 22.4 | 32.5 | 14.8 | 311.00000 | 86.600000 | 2.700000 | 1 | . 54 male | 13.4 | 42.600000 | 5.13 | 83.0 | 26.2 | 31.6 | 12.0 | 235.00000 | 84.864586 | 2.679310 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 33 female | 9.1 | 29.200000 | 4.38 | 61.9 | 19.3 | 31.1 | 16.1 | 410.00000 | 81.951429 | 2.200000 | 1 | . 152 female | 11.0 | 33.000000 | 5.29 | 62.4 | 20.8 | 33.3 | 16.0 | 335.00000 | 86.400000 | 3.000000 | 0 | . 0 female | 10.8 | 35.200000 | 5.12 | 68.7 | 21.2 | 30.8 | 13.4 | 309.00000 | 88.500000 | 2.600000 | 1 | . 157 male | 15.2 | 35.876404 | 6.09 | 72.6 | 25.0 | 34.4 | 12.2 | 332.27957 | 87.300000 | 2.600000 | 0 | . 98 female | 8.8 | 26.500000 | 3.81 | 69.6 | 23.0 | 33.0 | 14.7 | 180.00000 | 86.523291 | 2.588608 | 1 | . 100 rows × 12 columns . train_X = strat_train.drop(&#39;phenotype&#39;, axis=1) train_y = strat_train[&#39;phenotype&#39;] test_x = strat_test.drop(&#39;phenotype&#39;, axis=1) test_y = strat_test[&#39;phenotype&#39;] . one_hot1 = OneHotEncoder() transformer = ColumnTransformer([(&#39;one_hot&#39;, one_hot1, categorical_vars)], remainder= &#39;passthrough&#39;) trans_trainX = transformer.fit_transform(train_X) trans_trainX_df = pd.DataFrame(trans_trainX) . one_hot2 = OneHotEncoder() transformer = ColumnTransformer([(&#39;one_hot&#39;, one_hot2, categorical_vars)], remainder= &#39;passthrough&#39;) trans_testX = transformer.fit_transform(test_x) trans_testX_df = pd.DataFrame(trans_testX) . trans_testX_df.head() . 0 1 2 3 4 5 6 7 8 9 10 11 . 0 0.0 | 1.0 | 14.7 | 45.0 | 5.50 | 81.0 | 26.7 | 32.6 | 11.7 | 228.0 | 84.864586 | 2.67931 | . 1 0.0 | 1.0 | 12.1 | 41.5 | 5.50 | 75.5 | 23.8 | 31.6 | 14.1 | 266.0 | 84.864586 | 2.67931 | . 2 0.0 | 1.0 | 13.5 | 40.0 | 4.60 | 86.9 | 29.4 | 33.8 | 10.8 | 180.0 | 84.864586 | 2.67931 | . 3 0.0 | 1.0 | 14.3 | 43.6 | 4.92 | 88.7 | 29.0 | 32.7 | 13.5 | 265.0 | 84.864586 | 2.67931 | . 4 1.0 | 0.0 | 10.2 | 34.4 | 5.25 | 65.5 | 19.5 | 29.7 | 15.2 | 562.0 | 86.000000 | 2.90000 | . trans_trainX_df.head() . 0 1 2 3 4 5 6 7 8 9 10 11 . 0 1.0 | 0.0 | 11.3 | 35.900000 | 4.71 | 76.0 | 24.0 | 31.5 | 14.0 | 268.0 | 86.523291 | 2.588608 | . 1 1.0 | 0.0 | 11.6 | 36.000000 | 4.69 | 77.0 | 25.0 | 32.2 | 13.3 | 249.0 | 86.600000 | 2.500000 | . 2 1.0 | 0.0 | 13.8 | 35.876404 | 5.81 | 74.0 | 23.7 | 31.6 | 11.8 | 312.0 | 87.100000 | 2.600000 | . 3 0.0 | 1.0 | 11.8 | 36.000000 | 5.27 | 68.3 | 22.4 | 32.5 | 14.8 | 311.0 | 86.600000 | 2.700000 | . 4 0.0 | 1.0 | 13.4 | 42.600000 | 5.13 | 83.0 | 26.2 | 31.6 | 12.0 | 235.0 | 84.864586 | 2.679310 | . train_y.unique . &lt;bound method Series.unique of 181 0 171 0 95 1 41 1 54 1 .. 93 1 20 1 85 1 77 1 143 1 Name: phenotype, Length: 162, dtype: int64&gt; . using cross_val_score with stratified folds - https://stackoverflow.com/questions/59002684/forcing-sklearn-cross-val-score-to-use-stratified-k-fold . results . NameError Traceback (most recent call last) &lt;ipython-input-213-100f62972f2f&gt; in &lt;module&gt; -&gt; 1 results NameError: name &#39;results&#39; is not defined . from sklearn.ensemble import RandomForestClassifier model = RandomForestClassifier(verbose=True, oob_score=True, n_estimators=500, max_features=&#39;log2&#39;, n_jobs=-1, min_samples_leaf=4) model.fit(trans_trainX_df, train_y) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers. [Parallel(n_jobs=-1)]: Done 26 tasks | elapsed: 0.1s [Parallel(n_jobs=-1)]: Done 176 tasks | elapsed: 0.8s [Parallel(n_jobs=-1)]: Done 426 tasks | elapsed: 1.8s [Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 2.1s finished . RandomForestClassifier(max_features=&#39;log2&#39;, min_samples_leaf=4, n_estimators=500, n_jobs=-1, oob_score=True, verbose=True) . y_scores = model.predict(trans_trainX_df) . [Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers. [Parallel(n_jobs=12)]: Done 26 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 176 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 426 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 500 out of 500 | elapsed: 0.1s finished . from sklearn.metrics import classification_report print(classification_report(y_scores, train_y)) . precision recall f1-score support 0 0.57 1.00 0.72 25 1 1.00 0.86 0.93 137 accuracy 0.88 162 macro avg 0.78 0.93 0.83 162 weighted avg 0.93 0.88 0.89 162 . y_scores_valid = model.predict(trans_testX_df) . [Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers. [Parallel(n_jobs=12)]: Done 26 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 176 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 426 tasks | elapsed: 0.1s [Parallel(n_jobs=12)]: Done 500 out of 500 | elapsed: 0.1s finished . print(classification_report(y_scores_valid,test_y)) . precision recall f1-score support 0 0.18 0.67 0.29 3 1 0.97 0.76 0.85 38 accuracy 0.76 41 macro avg 0.57 0.71 0.57 41 weighted avg 0.91 0.76 0.81 41 . from sklearn.model_selection import GridSearchCV . . param_grid = [ {&#39;n_estimators&#39;: [100, 150, 200, 250, 300, 350, 400, 450, 500, 600], &#39;max_features&#39;: [0.5, &#39;sqrt&#39;, &#39;log2&#39;], &#39;min_samples_leaf&#39;:[4], &#39;n_jobs&#39;:[-1]}, {&#39;n_estimators&#39;: [100, 150, 200, 250, 300, 350, 400, 450, 500], &#39;max_features&#39;: [0.5, &#39;sqrt&#39;, &#39;log2&#39;], &#39;n_jobs&#39;:[-1]}, ] rf = RandomForestClassifier() grid_search = GridSearchCV(rf, param_grid, cv=5, scoring=&#39;accuracy&#39;, return_train_score=True) grid_search.fit(trans_trainX_df, train_y) . GridSearchCV(cv=5, estimator=RandomForestClassifier(), param_grid=[{&#39;max_features&#39;: [0.5, &#39;sqrt&#39;, &#39;log2&#39;], &#39;min_samples_leaf&#39;: [4], &#39;n_estimators&#39;: [100, 150, 200, 250, 300, 350, 400, 450, 500, 600], &#39;n_jobs&#39;: [-1]}, {&#39;max_features&#39;: [0.5, &#39;sqrt&#39;, &#39;log2&#39;], &#39;n_estimators&#39;: [100, 150, 200, 250, 300, 350, 400, 450, 500], &#39;n_jobs&#39;: [-1]}], return_train_score=True, scoring=&#39;accuracy&#39;) . grid_search.best_params_ . {&#39;max_features&#39;: &#39;log2&#39;, &#39;min_samples_leaf&#39;: 4, &#39;n_estimators&#39;: 200, &#39;n_jobs&#39;: -1} . model = RandomForestClassifier(verbose=True, oob_score=True, n_estimators=200, max_features=&#39;log2&#39;, n_jobs=-1, min_samples_leaf=4) model.fit(trans_trainX_df, train_y) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers. [Parallel(n_jobs=-1)]: Done 26 tasks | elapsed: 0.1s [Parallel(n_jobs=-1)]: Done 176 tasks | elapsed: 0.7s [Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 0.7s finished . RandomForestClassifier(max_features=&#39;log2&#39;, min_samples_leaf=4, n_estimators=200, n_jobs=-1, oob_score=True, verbose=True) . from sklearn.metrics import f1_score . f1_score(model.predict(trans_testX_df), test_y) . [Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers. [Parallel(n_jobs=12)]: Done 26 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 176 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed: 0.0s finished . 0.8695652173913044 . from sklearn.metrics import precision_score precision_score(model.predict(trans_testX_df), test_y) . [Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers. [Parallel(n_jobs=12)]: Done 26 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 176 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed: 0.0s finished . 1.0 . from sklearn.metrics import recall_score recall_score(model.predict(trans_testX_df), test_y) . [Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers. [Parallel(n_jobs=12)]: Done 26 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 176 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed: 0.0s finished . 0.7692307692307693 . y_scores = model.predict(trans_trainX_df) print(classification_report(y_scores, train_y)) . precision recall f1-score support 0 0.61 1.00 0.76 27 1 1.00 0.87 0.93 135 accuracy 0.90 162 macro avg 0.81 0.94 0.85 162 weighted avg 0.94 0.90 0.90 162 . [Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers. [Parallel(n_jobs=12)]: Done 26 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 176 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed: 0.0s finished . y_scores_valid = model.predict(trans_testX_df) print(classification_report(y_scores_valid,test_y)) . precision recall f1-score support 0 0.18 1.00 0.31 2 1 1.00 0.77 0.87 39 accuracy 0.78 41 macro avg 0.59 0.88 0.59 41 weighted avg 0.96 0.78 0.84 41 . [Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers. [Parallel(n_jobs=12)]: Done 26 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 176 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed: 0.0s finished . from sklearn.metrics import precision_recall_curve y_scores = model.predict_proba(trans_testX_df) #precision_recall_curve(test_y, y_scores) . [Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers. [Parallel(n_jobs=12)]: Done 26 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 176 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed: 0.0s finished . model1.classes_ . NameError Traceback (most recent call last) &lt;ipython-input-140-b81240a189d5&gt; in &lt;module&gt; -&gt; 1 model1.classes_ NameError: name &#39;model1&#39; is not defined . y_scores . array([[0.39250572, 0.60749428], [0.25320079, 0.74679921], [0.1713831 , 0.8286169 ], [0.17947367, 0.82052633], [0.489447 , 0.510553 ], [0.56838878, 0.43161122], [0.20219407, 0.79780593], [0.35415521, 0.64584479], [0.31536785, 0.68463215], [0.2188261 , 0.7811739 ], [0.3774125 , 0.6225875 ], [0.37452597, 0.62547403], [0.37466037, 0.62533963], [0.46889146, 0.53110854], [0.27090818, 0.72909182], [0.36832506, 0.63167494], [0.26114827, 0.73885173], [0.48415451, 0.51584549], [0.17540571, 0.82459429], [0.49270883, 0.50729117], [0.23196986, 0.76803014], [0.08120887, 0.91879113], [0.30504464, 0.69495536], [0.22847752, 0.77152248], [0.29418737, 0.70581263], [0.41770902, 0.58229098], [0.2760021 , 0.7239979 ], [0.40566932, 0.59433068], [0.45339157, 0.54660843], [0.3564696 , 0.6435304 ], [0.4670073 , 0.5329927 ], [0.34411017, 0.65588983], [0.16982323, 0.83017677], [0.19704983, 0.80295017], [0.35555874, 0.64444126], [0.37553992, 0.62446008], [0.38274799, 0.61725201], [0.38877291, 0.61122709], [0.50700195, 0.49299805], [0.27680689, 0.72319311], [0.45167561, 0.54832439]]) . y_scores = y_scores[:, 1] . precisions, recalls, thresholds = precision_recall_curve(test_y, y_scores) . def plot_precision_recall_vs_threshold(precisions, recalls, thresholds): plt.plot(thresholds, precisions[:-1], &quot;b--&quot;, label=&quot;Precision&quot;) plt.plot(thresholds, recalls[:-1], &quot;g-&quot;, label=&quot;Recall&quot;) plt.legend(loc=&quot;upper left&quot;) # highlight the threshold and add the legend, axis label, and grid . plot_precision_recall_vs_threshold(precisions, recalls, thresholds) . from sklearn.metrics import roc_curve fpr, tpr, thresholds = roc_curve(test_y, y_scores) . def plot_roc_curve(fpr, tpr, label=None, figure=(12,10)): plt.plot(fpr, tpr, linewidth=2, label=label) plt.plot([0, 1], [0, 1], &#39;k--&#39;) # Dashed diagonal # Add axis labels and grid #plt.figure(figsize=figure) plt.xlabel(&#39;True Positive Rate(Recall)&#39;) plt.ylabel(&quot;False Positive Rate(1-specificity)&quot;) plt.legend() plot_roc_curve(fpr, tpr) . No handles with labels found to put in legend. . from sklearn.metrics import roc_auc_score roc_auc_score(test_y, y_scores) . 0.7303030303030303 . import joblib joblib.dump(model1, &quot;thal_rf200.pkl&quot;) # and later... #my_model_loaded = joblib.load(&quot;my_model.pkl&quot;) . [&#39;thal_rf200.pkl&#39;] . from sklearn.metrics import confusion_matrix from sklearn.metrics import plot_confusion_matrix confused = confusion_matrix(model.predict(trans_testX_df), test_y) confused . [Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers. [Parallel(n_jobs=12)]: Done 26 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 176 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed: 0.0s finished . array([[ 2, 0], [ 9, 30]], dtype=int64) . fig, ax = plt.subplots(figsize=(10, 10)) plot_confusion_matrix (model, trans_testX_df, test_y, cmap=plt.cm.Blues, display_labels=[&#39;normals&#39;, &#39;alpha carriers&#39;], ax=ax) . [Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers. [Parallel(n_jobs=12)]: Done 26 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 176 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed: 0.0s finished . &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x20d3a29a308&gt; . from imblearn.over_sampling import SMOTE sm = SMOTE(random_state = 2) X_train_res, y_train_res = sm.fit_sample(trans_trainX_df, train_y) . y_train_res.value_counts() . 0 118 1 118 Name: phenotype, dtype: int64 . model = RandomForestClassifier(verbose=True, oob_score=True, n_estimators=400, max_features=0.5, n_jobs=-1, min_samples_leaf=4) model.fit(X_train_res, y_train_res) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers. [Parallel(n_jobs=-1)]: Done 26 tasks | elapsed: 0.1s [Parallel(n_jobs=-1)]: Done 176 tasks | elapsed: 0.6s [Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 1.3s finished . RandomForestClassifier(max_features=0.5, min_samples_leaf=4, n_estimators=400, n_jobs=-1, oob_score=True, verbose=True) . https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/ print(classification_report(model.predict(X_train_res), y_train_res)) . [Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers. [Parallel(n_jobs=12)]: Done 26 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 176 tasks | elapsed: 0.0s . precision recall f1-score support 0 0.97 0.97 0.97 117 1 0.97 0.97 0.97 119 accuracy 0.97 236 macro avg 0.97 0.97 0.97 236 weighted avg 0.97 0.97 0.97 236 . [Parallel(n_jobs=12)]: Done 400 out of 400 | elapsed: 0.2s finished . print(classification_report(model.predict(trans_testX_df), test_y)) . precision recall f1-score support 0 0.73 0.47 0.57 17 1 0.70 0.88 0.78 24 accuracy 0.71 41 macro avg 0.71 0.67 0.67 41 weighted avg 0.71 0.71 0.69 41 . [Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers. [Parallel(n_jobs=12)]: Done 26 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 176 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 400 out of 400 | elapsed: 0.0s finished . fig, ax = plt.subplots(figsize=(10, 10)) plot_confusion_matrix (model, trans_testX_df, test_y, cmap=plt.cm.Blues, display_labels=[&#39;normals&#39;, &#39;alpha carriers&#39;], ax=ax) . [Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers. [Parallel(n_jobs=12)]: Done 26 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 176 tasks | elapsed: 0.0s [Parallel(n_jobs=12)]: Done 400 out of 400 | elapsed: 0.0s finished . &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x20d3bbe2388&gt; . from xgboost import XGBClassifier as xgb . xgbmodel = xgb() xgbmodel.fit(X_train_res, y_train_res) . XGBClassifier() . print(classification_report(xgbmodel.predict(X_train_res), y_train_res)) . precision recall f1-score support 0 0.99 1.00 1.00 117 1 1.00 0.99 1.00 119 accuracy 1.00 236 macro avg 1.00 1.00 1.00 236 weighted avg 1.00 1.00 1.00 236 . print(classification_report(xgbmodel.predict(trans_testX_df), test_y)) . precision recall f1-score support 0 0.64 0.54 0.58 13 1 0.80 0.86 0.83 28 accuracy 0.76 41 macro avg 0.72 0.70 0.71 41 weighted avg 0.75 0.76 0.75 41 . param_grid = [ {&#39;learning_rate&#39;: [0.01, 0.05, 0.1, 0.15, 0.2, 0.25], &#39;max_features&#39;: [0.5, &#39;sqrt&#39;, &#39;log2&#39;], &#39;min_samples_leaf&#39;:[4], &#39;n_jobs&#39;:[-1]}, {&#39;n_estimators&#39;: [100, 150, 200, 250, 300, 350, 400, 450, 500], &#39;max_features&#39;: [0.5, &#39;sqrt&#39;, &#39;log2&#39;], &#39;n_jobs&#39;:[-1]}, ] rf = RandomForestClassifier() grid_search = GridSearchCV(rf, param_grid, cv=5, scoring=&#39;accuracy&#39;, return_train_score=True) grid_search.fit(trans_trainX_df, train_y) . from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold # define model model2 = RandomForestClassifier() # evaluate pipeline cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(model2, X, y, scoring=&#39;roc_auc&#39;, cv=cv, n_jobs=-1) print(&#39;Mean ROC AUC: %.3f&#39; % mean(scores)) .",
            "url": "https://truthdead.github.io/nival-blog/2022/05/31/Thal.html",
            "relUrl": "/2022/05/31/Thal.html",
            "date": " • May 31, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://truthdead.github.io/nival-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://truthdead.github.io/nival-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://truthdead.github.io/nival-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://truthdead.github.io/nival-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}